from utils import *
from inits import *
from activations import *
from losses import *
from optim import *
from layers import *
from preprocessing import *
from copy import deepcopy
import math

losses = {"mse": MSE_loss(), "cross_entropy": Cross_entropy_loss()}

class NeuralNetwork():
    def __init__(self, layers, batch, num_epochs, optimizer, w_init, targets, loss_fn, validate=False, validation_inputs = None, val_target = None, optim_params = None, wandb_=False):
        self.layers = layers
        self.batch = batch
        self.num_epochs = num_epochs
        if optimizer == "sgd":
            self.optimizer = SGD()
        elif optimizer == "gdm":
            self.optimizer = GDMomentum()
        elif optimizer == "gdn":
            self.optimizer = GDNesterov()
        elif optimizer == "rmsprop":
            self.optimizer = RMSProp()
        elif optimizer == "adam":
            self.optimizer = Adam()
        elif optimizer == "nadam":
            self.optimizer = Nadam()
        self.w_init = w_init
        self.targets = targets
        self.loss_fn = loss_fn
        self.loss_fx = losses[loss_fn]
        self.num_batches = math.ceil(self.targets.shape[1]/batch)
        self.wandb_ = wandb_

        self.validate = validate
        if validate is not None:
            self.X_val = validation_inputs
            self.layers[0].val_h_x = validation_inputs
            self.val_target = val_target

        self.loss = []
        self.accuracy = []
        self.val_loss = []
        self.val_accuracy = []
        
        self.init_params(optimizer,optim_params)

    def init_params(self, optimizer, optim_params):
        input_size = self.layers[0].X.shape[1]

        if self.w_init == "XavierInit":
            init=XavierInit()
            self.layers[0].W, self.layers[0].b = init.initialize(self.layers[0].X.shape[1], self.layers[0].size)
        elif self.w_init == "RandomInit":
            init=RandomInit()
            self.layers[0].W, self.layers[0].b = init.initialize(self.layers[0].X.shape[1], self.layers[0].size)

        for layer in self.layers[1:]:
            layer.W_size = layer.size, input_size

            if optimizer == "sgd":
                layer.W_optimizer = deepcopy(SGD())
                layer.b_optimizer = deepcopy(SGD())
            elif optimizer == "gdm":
                layer.W_optimizer = deepcopy(GDMomentum())
                layer.b_optimizer = deepcopy(GDMomentum())
            elif optimizer == "gdn":
                layer.W_optimizer = deepcopy(GDNesterov())
                layer.b_optimizer = deepcopy(GDNesterov())
            elif optimizer == "rmsprop":
                layer.W_optimizer = deepcopy(RMSProp())
                layer.b_optimizer = deepcopy(RMSProp())
            elif optimizer == "adam":
                layer.W_optimizer = deepcopy(Adam())
                layer.b_optimizer = deepcopy(Adam())
            elif optimizer == "nadam":
                layer.W_optimizer = deepcopy(Nadam())
                layer.b_optimizer = deepcopy(Nadam())

            if not hasattr(layer, "W") or layer.W is None:
                if self.w_init == "XavierInit":
                    init=XavierInit()
                    layer.W, layer.b = init.initialize(input_size, layer.size)
                elif self.w_init == "RandomInit":
                    init=RandomInit()
                    layer.W, layer.b = init.initialize(input_size, layer.size)

            if optim_params:
                layer.W_optimizer.set_params(optim_params)
                layer.b_optimizer.set_params(optim_params)

            input_size = layer.size
            
        self.layers[-1].W_size = self.layers[-1].size, input_size



    def forward_pass(self):
        self.layers[0].h_x = self.layers[0].x
        if self.validate:
            self.layers[0].h_x = self.layers[0].X
        
        for i in range(1, len(self.layers)):
            self.layers[i].g_x = np.dot(self.layers[i].h_x.T, self.layers[i].W.T).T + self.layers[i].b
            self.layers[i].h_x = self.layers[i].activation.forward(self.layers[i].g_x)
            if self.validate:
                self.layers[i].val_g_x = np.dot(self.layers[i].val_h_x.T, self.layers[i-1].W.T).T + self.layers[i].b
                self.layers[i].val_h_x = self.layers[i].activation.forward(self.layers[i].val_g_x)
        
        if self.loss_fn == "cross_entropy":
            self.layers[-1].A = Softmax().value(self.layers[-1].h_x)
            if self.validate:
                self.layers[-1].val_A = Softmax().value(self.layers[-1].val_h_x)
        else:
            self.layers[-1].A = self.layers[-1].h_x
            if self.validate:
                self.layers[-1].val_A = self.layers[-1].val_h_x


    def backward_pass(self, loss_fx):
        stop = 0
        self.eta=[]
        self.load = []#
        self.accuracy=[]#
        self.loss=[]#
        self.val_accuracy = []#
        for ep in range(self.num_epochs):
            self.eta.append(self.layers[-1].W_optimizer.lr)
            self.forward_pass()
            train_loss = self.loss_fx.loss(self.layers[-1].A, self.targets)
            train_accuracy = self.calculate_accuracy(self.layers[-1].A, self.targets)
            self.load.append(train_loss)
            self.accuracy.append(train_accuracy)

            if self.validate:
                val_loss = self.loss_fx.loss(self.layers[-1].val_A, self.val_target)
                val_accuracy = self.calculate_accuracy(self.layers[-1].val_A, self.val_target)
                self.val_loss.append(val_loss)
                self.val_accuracy.append(val_accuracy)
            
            if self.wandb_:
                log_dict = {
                    "step": ep,
                    "accuracy": train_accuracy,
                    "loss": train_loss
                }
                if self.validate:
                    log_dict.update({
                        "val_loss": val_loss,
                        "val_accuracy": val_accuracy
                    })
                wandb.log(log_dict)

            try:
                if len(self.loss)>1 and self.loss[-1]>self.loss[-2]:
                    for layer in self.layers[1:]:
                        layer.W_optimizer.lr = layer.W_optimizer.lr*0.5
                        layer.b_optimizer.lr = layer.b_optimizer.lr*0.5
                    stop+=1
                    if stop > 3:
                        print("Early Stopping")
                        break
            except:
                pass

            #Mini-batch gradient descent
            for b in range(self.num_batches):
                batch_start = b* self.batch
                batch_end = (b+1)*self.batch
                t_batch = self.targets[:, batch_start:batch_end]
                # self.y_batch = self.layers[-1].y[:,b*self.batch:(b+1)*self.batch]
                self.forward_batch(batch_start, batch_end)

                if self.loss_fx == "cross_entropy":
                    self.layers[-1].dA = self.loss_fx.differential(self.layers[-1].A[:, batch_start:batch_end], self.t_batch)
                else:
                    self.layers[-1].dA = -2 * (t_batch - self.layers[-1].A[:, batch_start:batch_end])

                self.layers[-1].dH = self.layers[-1].dA * self.layers[-1].activation.backward(self.layers[-1].g_x[:, batch_start:batch_end])
                
                self.layers[-1].dW = np.dot(self.layers[-1].dH, self.layers[-2].h_x[:, batch_start:batch_end])
                self.layers[-1].db = np.sum(self.layers[-1].dH, axis=1).reshape(-1,1)

                self.layers[-1].W_update = self.layers[-1].W_optimizer.get_iter(self.layers[-1].dW)
                self.layers[-1].b_update = self.layers[-1].b_optimizer.get_iter(self.layers[-1].db)
                
                for layer in self.layers[1:]:
                    layer.W -= layer.W_update
                    layer.b -= layer.b_update
                
                for layer in self.layers[1:]:
                    layer.W = layer.W - layer.W_new
                    layer.b = layer.b - layer.b_new

                self.forward_pass()

    def forward_batch(self, batch_start, batch_end):
        self.layers[0].h_x_batch = self.layers[0].X[:, batch_start:batch_end]

        for i in range(1, len(self.layers)):
            self.layers[i].g_x_batch = np.dot(self.layers[i-1].h_x_batch.T, self.layers[i].W.T).T + self.layers[i].b
            self.layers[i].h_x_batch = self.layers[i].activation.forward(self.layers[i].g_x_batch)

#mod(pred, targets)
    def calculate_accuracy(self, predictions, targets):
        if self.loss_fn == "cross_entropy":
            pred_labels = np.argmax(predictions, axis=0)
            true_labels = np.argmax(targets, axis=0)
            return np.mean(pred_labels == true_labels)
        else:
            return 1.0 - np.mean(np.abs(predictions - targets))
    def predict(self, X):
        self.layers[0].h_x = X
        
        for i in range(1, len(self.layers)):
            self.layers[i].g_x = np.dot(self.layers[i].h_x.T, self.layers[i].W.T).T + self.layers[i].b
            self.layers[i].h_x = self.layers[i].activation.forward(self.layers[i].g_x)
        
        if self.loss_fn == "cross_entropy":
            return Softmax().value(self.layers[-1].h_x)
        else:
            return self.layers[-1].h_x
    
    def about(self):
        print("Layers: ", len(self.layers))
        print("Loss: ", self.loss)
        print("Accuracy: ", self.accuracy)
        print("Val Loss: ", self.val_loss)
        print("Val Accuracy: ", self.val_accuracy)

    def check_(self):
        for i, layer in enumerate(self.layers):
            print(f"Layer {i}:")
            if hasattr(layer, 'size'):
                print(f"Size: {layer.size}")
            if i>0:
                if hasattr(layer, 'W') and hasattr(layer, 'b'):
                    print(f"Weights shape: {layer.W.shape}")
                    print(f"Biases shape: {layer.b.shape}")
                    # print(f"Activation: {layer.activation.name}") 
                else:
                    print("Warning: Layer weights or biases not found.")

        print("\nData dimensions:")
        print(f"Input features: {self.layers[0].X.shape[0]}")
        print(f"Target shape: {self.targets.shape}")

        if self.validate:
            print("\nValidate data:")
            print(f"Validation features: {self.X_val.shape}")
            print(f"Validation targets: {self.val_target.shape}")

        # print(f"\nOptimizer: {self.optimizer}")
        print(f"Weight initialisation: {self.w_init}")
        print(f"Loss function: {self.loss_fn}")
        print(f"Batch size: {self.batch}")
        print(f"Number of epochs: {self.num_epochs}")
        print(f"Number of batches: {self.num_batches}")

        # try:
        #     self.forward_pass()
        #     print()

